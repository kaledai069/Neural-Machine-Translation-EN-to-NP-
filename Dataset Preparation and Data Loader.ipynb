{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "542b0742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x25643c51e50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pathlib\n",
    "import re\n",
    "import unicodedata\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "001b0302",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'MAX_VOCAB_SIZE': 13000,\n",
    "    'BATCH_SIZE': 8,\n",
    "    'raw_dataset_path': './dataset/por.txt',\n",
    "    'MAX_SEQ_LEN': 16,\n",
    "    'BUFFER_SIZE': 1000,\n",
    "    'UNITS': 256,\n",
    "    'EPOCHS': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ffd6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "dataset_path = pathlib.Path(config['raw_dataset_path'])\n",
    "text_data = dataset_path.read_text(encoding = 'utf-8')\n",
    "\n",
    "lines = text_data.splitlines()\n",
    "pairs = [line.split('\\t') for line in lines][:1200]\n",
    "\n",
    "context_en = np.array([context for context, target, _ in pairs])\n",
    "target_por = np.array([target for context, target, _ in pairs])\n",
    "\n",
    "sentences = np.array((context_en, target_por))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b12512c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['go', 'on', '.'], ['siga', 'em', 'frente', '.'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^ a-z.?!,¿]\", \"\", text)\n",
    "    text = re.sub(r\"([.?!,¿])\", r\" \\1 \", text)\n",
    "    text = text.strip()\n",
    "    return text.split()\n",
    "\n",
    "tokenizer(context_en[34]), tokenizer(target_por[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d3e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a vocabulary\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold, max_vocab_size):\n",
    "        # maintain two different mappings\n",
    "        self.itos = {0: '[PAD]', 1: '[SOS]', 2: '[EOS]', 3: '[UNK]'}\n",
    "        self.stoi = {'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, '[UNK]': 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        \n",
    "        self.pad_id = self.stoi['[PAD]']\n",
    "        self.sos_id = self.stoi['[SOS]']\n",
    "        self.eos_id = self.stoi['[EOS]']\n",
    "        self.oov_id = self.stoi['[UNK]']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        return self.stoi\n",
    "\n",
    "    def token_to_ids(self, tokens):\n",
    "        if isinstance(tokens, str): # handle a single word or sentence here\n",
    "            token_list = self.tokenizer(tokens)\n",
    "            return [self.stoi[t] if t in self.stoi else self.stoi['[UNK]'] for t in token_list]\n",
    "\n",
    "        elif isinstance(tokens, list):\n",
    "            return [self.stoi[t] if t in self.stoi else self.stoi['[UNK]'] for t in tokens]\n",
    "        \n",
    "        else:\n",
    "            raise TypeError(\"Input must be either String or List of words.\")\n",
    "\n",
    "    def ids_to_token(self, ids):\n",
    "        return [self.itos[id] for id in ids]\n",
    "\n",
    "    # building vocab with the input sentence list\n",
    "    def adapt(self, sentences, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        idx = len(self.itos)\n",
    "        token_freqs = {}\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for token in self.tokenizer(sentence):\n",
    "                if token not in self.stoi:\n",
    "                    token_freqs[token] = 1\n",
    "                else:\n",
    "                    token_freqs[token] += 1\n",
    "                \n",
    "                if (token_freqs[token] == self.freq_threshold) and (idx < self.max_vocab_size):\n",
    "                    self.itos[idx] = token\n",
    "                    self.stoi[token] = idx\n",
    "                    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a7e967b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(419, 800)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# english vocabulary\n",
    "en_vocab = Vocabulary(freq_threshold = 1, max_vocab_size = config['MAX_VOCAB_SIZE'])\n",
    "en_vocab.adapt(context_en, tokenizer)\n",
    "\n",
    "# portuguese vocabulary\n",
    "por_vocab = Vocabulary(freq_threshold = 1, max_vocab_size = config['MAX_VOCAB_SIZE'])\n",
    "por_vocab.adapt(target_por, tokenizer)\n",
    "\n",
    "en_vocab.vocab_size(), por_vocab.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8c2901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here I am. ---------> Aqui estou.\n",
      "\n",
      "Encoder Input IDs: \n",
      "[1, 194, 20, 62, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Pre-Attention Decoder Input IDs (Shifted to the Right): \n",
      "[1, 402, 47, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Post-Attention Decoder Input IDs: \n",
      "[402, 47, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_idx = 789\n",
    "en_translation = context_en[test_idx]\n",
    "por_translation = target_por[test_idx]\n",
    "\n",
    "print(en_translation, '--------->', por_translation)\n",
    "\n",
    "max_seq_len = 16\n",
    "context_tokens = en_vocab.token_to_ids(en_translation)\n",
    "target_tokens = por_vocab.token_to_ids(por_translation)\n",
    "\n",
    "print(\"\\nEncoder Input IDs: \")\n",
    "print([en_vocab.sos_id] + context_tokens + [en_vocab.eos_id] + (max_seq_len - len(context_tokens) - 2) * [en_vocab.pad_id])\n",
    "print(\"\\nPre-Attention Decoder Input IDs (Shifted to the Right): \")\n",
    "print([en_vocab.sos_id] + target_tokens + [en_vocab.eos_id] + (max_seq_len - len(context_tokens) - 2) * [en_vocab.pad_id])\n",
    "print(\"\\nPost-Attention Decoder Input IDs: \")\n",
    "print(target_tokens + [en_vocab.eos_id] + (max_seq_len - len(context_tokens) - 1) * [en_vocab.pad_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84839390",
   "metadata": {},
   "source": [
    "### Neural Machine Translation Custom Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feee94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMT_dataset(Dataset):\n",
    "    def __init__(self, translation_pairs, tokenizer, vocabularies, max_seq_len, device = 'cpu'):\n",
    "        print(translation_pairs.shape)\n",
    "        self.translation_pairs = translation_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.en_vocab, self.por_vocab = vocabularies\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device\n",
    "\n",
    "        # for convenience \n",
    "        self.sos_id = self.en_vocab.sos_id\n",
    "        self.eos_id = self.en_vocab.eos_id\n",
    "        self.pad_id = self.en_vocab.pad_id\n",
    "        self.oov_id = self.en_vocab.oov_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.translation_pairs.shape[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        req_pair = self.translation_pairs[:, idx]\n",
    "        en_translation, por_translation = req_pair\n",
    "\n",
    "        context_tokens = self.en_vocab.token_to_ids(en_translation)\n",
    "        target_tokens = self.por_vocab.token_to_ids(por_translation)\n",
    "\n",
    "        # encoder input tokens\n",
    "        encoder_input = (\n",
    "            [self.sos_id] + \n",
    "            context_tokens + \n",
    "            [self.eos_id] + \n",
    "            (self.max_seq_len - len(context_tokens) - 2) * [self.pad_id]\n",
    "            )\n",
    "        \n",
    "        # pre-attention decoder input tokens\n",
    "        pre_decoder_input = (\n",
    "            [self.sos_id] + \n",
    "            target_tokens + \n",
    "            [self.eos_id] + \n",
    "            (self.max_seq_len - len(target_tokens) - 2) * [self.pad_id] \n",
    "        )\n",
    "\n",
    "        # post-attention decoder output tokens\n",
    "        post_decoder_output = (\n",
    "            target_tokens + \n",
    "            [self.eos_id] + \n",
    "            (self.max_seq_len - len(target_tokens) - 1) * [self.pad_id]\n",
    "        )\n",
    "\n",
    "        encoder_input_tensor = torch.tensor(encoder_input[:max_seq_len], dtype = torch.long).to(self.device)\n",
    "        pre_decoder_input_tensor = torch.tensor(pre_decoder_input[:max_seq_len], dtype = torch.long).to(self.device)\n",
    "        post_decoder_output_tensor = torch.tensor(post_decoder_output[:max_seq_len], dtype = torch.long).to(self.device)\n",
    "\n",
    "        return encoder_input_tensor, pre_decoder_input_tensor, post_decoder_output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a66997a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 1040), (2, 160))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and val_dataset\n",
    "is_train = np.random.uniform(size = (sentences.shape[-1],)) < 0.85\n",
    "train_raw_set = sentences[:, is_train]\n",
    "val_raw_set = sentences[:, ~is_train]\n",
    "\n",
    "train_raw_set.shape, val_raw_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "974817a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1040)\n",
      "(2, 160)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NMT_dataset(train_raw_set, tokenizer, (en_vocab, por_vocab), config['MAX_SEQ_LEN'])\n",
    "val_dataset = NMT_dataset(val_raw_set, tokenizer, (en_vocab, por_vocab), config['MAX_SEQ_LEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a832fb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 1040/1040 [00:00<00:00, 19423.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'encoder_in': 16, 'target_in': 16, 'target_out': 16}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_dict = {\n",
    "    'encoder_in': 0, \n",
    "    'target_in': 0, \n",
    "    'target_out': 0\n",
    "}\n",
    "context_target_token_lengths = {\n",
    "    'encoder_in': [], \n",
    "    'target_in': [], \n",
    "    'target_out': []\n",
    "}\n",
    "\n",
    "for data in tqdm(train_dataset, ncols = 100):\n",
    "    encoder_in, target_in, target_out = data\n",
    "    context_target_token_lengths['encoder_in'].append(len(encoder_in))\n",
    "    context_target_token_lengths['target_in'].append(len(target_in))\n",
    "    context_target_token_lengths['target_out'].append(len(target_out))\n",
    "    \n",
    "    if len(encoder_in) > max_len_dict['encoder_in']:\n",
    "        max_len_dict['encoder_in'] = len(encoder_in)\n",
    "        \n",
    "    if len(target_in) > max_len_dict['target_in']:\n",
    "        max_len_dict['target_in'] = len(target_in)\n",
    "        \n",
    "    if len(target_out) > max_len_dict['target_out']:\n",
    "        max_len_dict['target_out'] = len(target_out)\n",
    "\n",
    "max_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bf2dfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(16, 1040)], [(16, 1040)], [(16, 1040)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(context_target_token_lengths['encoder_in']).most_common(1), Counter(context_target_token_lengths['target_in']).most_common(1), Counter(context_target_token_lengths['target_out']).most_common(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca0aa50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # pad sequences according to the maximum len in a batch, and then standardize all the batch inputs to same length\n",
    "    contexts, target_ins, target_outs = zip(*batch)\n",
    "    max_len = max([len(x) for x in contexts])\n",
    "\n",
    "    # padding sequences\n",
    "    padded_contexts = [torch.nn.functional.pad(x, (0, max_len - len(x))) for x in contexts]\n",
    "    padded_target_ins = [torch.nn.functional.pad(x, (0, max_len - len(x))) for x in target_ins]\n",
    "    padded_target_outs = [torch.nn.functional.pad(x, (0, max_len - len(x))) for x in target_outs]\n",
    "\n",
    "    padded_contexts = torch.stack(padded_contexts)\n",
    "    padded_target_ins = torch.stack(padded_target_ins)\n",
    "    padded_target_outs = torch.stack(padded_target_outs)\n",
    "\n",
    "    return padded_contexts, padded_target_ins, padded_target_outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "342a41de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = config['BATCH_SIZE'], \n",
    "    shuffle = True,\n",
    "#     collate_fn = collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size = config['BATCH_SIZE'], \n",
    "    shuffle = False,\n",
    "#     collate_fn = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "118950b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1040, 160)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader) * config['BATCH_SIZE'], len(val_loader) * config['BATCH_SIZE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69c36198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 256])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # input embedding\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = vocab_size,\n",
    "            embedding_dim = units,\n",
    "            padding_idx = 0 \n",
    "        )\n",
    "        \n",
    "        # bi-directional LSTM\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = units,\n",
    "            hidden_size = units,\n",
    "            batch_first = True,\n",
    "            bidirectional = True\n",
    "        )\n",
    "\n",
    "    def forward(self, context):\n",
    "        x = self.embedding(context)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = x[:, :, :self.rnn.hidden_size] + x[:, :, self.rnn.hidden_size:]\n",
    "\n",
    "        return x\n",
    "    \n",
    "vocab_size = config['MAX_VOCAB_SIZE']\n",
    "units = config['UNITS']\n",
    "\n",
    "encoder = Encoder(vocab_size, units)\n",
    "\n",
    "input_tensor = next(iter(train_dataset))\n",
    "input_tensor = input_tensor[0].unsqueeze(0)\n",
    "output = encoder(input_tensor)\n",
    "print(output.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f249a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 256])\n"
     ]
    }
   ],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, units):\n",
    "\n",
    "        super(CrossAttention, self).__init__()\n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim = units,  # the size of Q, K, V dims is the embedding dimension\n",
    "            num_heads = 1,  \n",
    "            batch_first = True  # (batch_size, sequence_length, embedding_dim) either (seq_len, batch_size, embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(units)\n",
    "        \n",
    "        # to accumulate the inputs from encoder output (encoder_in) and the pre-attention decoder (target_in)\n",
    "        self.add = nn.ModuleList([nn.Linear(units, units) for _ in range(2)])\n",
    "\n",
    "    def forward(self, context, target):\n",
    "        # query is the target_in from the pre-attention decoder, while key/value are the context from the encoded context from the encoder\n",
    "        attn_output, _ = self.multihead_attn(query = target, key = context, value = context)\n",
    "        \n",
    "        x = self.add[0](target) + self.add[1](attn_output)\n",
    "        return self.layernorm(x)\n",
    "\n",
    "\n",
    "units = config['UNITS']\n",
    "\n",
    "cross_attention = CrossAttention(units)\n",
    "context_tensor = torch.randn(8, 16, units)  \n",
    "target_tensor = torch.randn(8, 16, units)    \n",
    "output = cross_attention(context_tensor, target_tensor)\n",
    "print(output.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90b94864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 13000])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, units, padding_idx = 0)\n",
    "        self.pre_attention_rnn = nn.LSTM(units, units, batch_first = True)\n",
    "        self.attention = CrossAttention(units)\n",
    "        self.post_attention_rnn = nn.LSTM(units, units, batch_first = True)\n",
    "        self.output_layer = nn.Linear(units, vocab_size)\n",
    "\n",
    "    def forward(self, context, target, state = None, return_state = False):\n",
    "        \n",
    "        x = self.embedding(target)\n",
    "        \n",
    "        # pre-attention-LSTM (decoder)\n",
    "        if state is None:\n",
    "            x, (hidden_state, cell_state) = self.pre_attention_rnn(x)\n",
    "        else:\n",
    "            x, (hidden_state, cell_state) = self.pre_attention_rnn(x, state)\n",
    "        \n",
    "        # cross-attention between pre-attention-LSTM output and the encoded context\n",
    "        x = self.attention(context, x)\n",
    "        \n",
    "        # post-attention-LSTM (decoder)\n",
    "        x, _ = self.post_attention_rnn(x)\n",
    "\n",
    "        # last linear (dense) layer\n",
    "        logits = self.output_layer(x)\n",
    "        logits = F.log_softmax(logits, dim = -1)\n",
    "\n",
    "        if return_state:\n",
    "            return logits, (hidden_state, cell_state)\n",
    "\n",
    "        return logits\n",
    "\n",
    "vocab_size = config['MAX_VOCAB_SIZE']\n",
    "units = config['UNITS']\n",
    "decoder = Decoder(vocab_size, units)\n",
    "\n",
    "context_tensor = torch.randn(8, 15, units)  \n",
    "target_tensor = torch.randint(0, vocab_size, (8, 16)).long() \n",
    "\n",
    "output = decoder(context_tensor, target_tensor)\n",
    "print(output.shape) # (batch_size, max_seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1aa69111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMT_Translator(nn.Module):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        super(NMT_Translator, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size, units)\n",
    "        self.decoder = Decoder(vocab_size, units)\n",
    "        \n",
    "    def forward(self, context, target):\n",
    "        encoded_context = self.encoder(context)\n",
    "        logits = self.decoder(encoded_context, target) # this target here is the target_in (target_out is used for training)\n",
    "        \n",
    "        return torch.argmax(logits, dim = -1) # (batch_size, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea725999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 15]) torch.Size([8, 16])\n",
      "torch.Size([8, 16])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = config['MAX_VOCAB_SIZE']\n",
    "units = config['UNITS']\n",
    "\n",
    "\n",
    "translator = NMT_Translator(vocab_size, units)\n",
    "\n",
    "context_tensor = torch.randint(0, vocab_size, (8, 15)).long()  # (batch_size, seq_len, units)\n",
    "target_tensor = torch.randint(0, vocab_size, (8, 16)).long()  # (batch_size, seq_len)\n",
    "print(context_tensor.shape, target_tensor.shape)\n",
    "\n",
    "\n",
    "output = translator(context_tensor, target_tensor)\n",
    "print(output.shape)  # (batch_size, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8350f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedLoss, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss(reduction = 'none')\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "#         y_pred = y_pred.argmax(dim=-1).float()\n",
    "        \n",
    "        mask = (y_true != 0).float()\n",
    "        y_pred *= mask\n",
    "        loss = self.loss_fn(y_pred, y_true.float())\n",
    "#         print(y_pred)\n",
    "#         print(y_true)\n",
    "#         print(loss)\n",
    "#         print(new_thing)\n",
    "#         loss *= mask\n",
    "        return loss.sum() / mask.sum()\n",
    "\n",
    "class MaskedAcc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedAcc, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "#         y_pred = y_pred.argmax(dim=-1)\n",
    "\n",
    "        mask = (y_true != 0).float()\n",
    "        correct = (y_true == y_pred).float() * mask\n",
    "        return correct.sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c6b929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = config['MAX_VOCAB_SIZE']\n",
    "units = config['UNITS']\n",
    "model = NMT_Translator(vocab_size, units)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "masked_loss_fn = MaskedLoss()\n",
    "masked_acc_fn = MaskedAcc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9783990b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training loss: 559217.0648, accuracy: 0.0000\n",
      "Validation loss: 570672.9576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Training loss: 559789.0902, accuracy: 0.0000\n",
      "Validation loss: 570672.9576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Training loss: 556748.2055, accuracy: 0.0000\n",
      "Validation loss: 570672.9576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Training loss: 558044.8694, accuracy: 0.0000\n",
      "Validation loss: 570672.9576\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parzi\\AppData\\Local\\Temp\\ipykernel_9992\\3763824845.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(config['EPOCHS']):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['EPOCHS']}\", leave = False)\n",
    "    for batch in train_loader_tqdm:\n",
    "        \n",
    "        # encoder input, pre-attention-decoder input and post-attention-decoder target output\n",
    "        context, target_in, target_out = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(context, target_in)\n",
    "\n",
    "        loss = masked_loss_fn(output.float(), target_out.float())\n",
    "        acc = masked_acc_fn(output.float(), target_out.float())\n",
    "        \n",
    "        loss.requires_grad = True\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item()\n",
    "        train_loader_tqdm.set_postfix(loss = loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_acc = total_acc / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{config['EPOCHS']}\")\n",
    "    print(f\"Training loss: {avg_loss:.4f}, accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            context, target_in, target_out = batch\n",
    "            output = model(context, target_in)\n",
    "            loss = masked_loss_fn(output.float(), target_out.float())\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= 3:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea7198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac1666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a163a006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c76397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
