{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "542b0742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pathlib\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "059cb014",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'MAX_VOCAB_SIZE': 13000,\n",
    "    'BATCH_SIZE': 8,\n",
    "    'raw_dataset_path': './dataset/por.txt',\n",
    "    'MAX_SEQ_LEN': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ffd6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "dataset_path = pathlib.Path(config['raw_dataset_path'])\n",
    "text_data = dataset_path.read_text(encoding = 'utf-8')\n",
    "\n",
    "lines = text_data.splitlines()\n",
    "pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "context_en = np.array([context for context, target, _ in pairs])\n",
    "target_por = np.array([target for context, target, _ in pairs])\n",
    "\n",
    "sentences = list(zip(context_en, target_por))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b12512c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['go', 'on', '.'], ['siga', 'em', 'frente', '.'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^ a-z.?!,¿]\", \"\", text)\n",
    "    text = re.sub(r\"([.?!,¿])\", r\" \\1 \", text)\n",
    "    text = text.strip()\n",
    "    return text.split()\n",
    "\n",
    "tokenizer(context_en[34]), tokenizer(target_por[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95d3e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a vocabulary\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold, max_vocab_size):\n",
    "        # maintain two different mappings\n",
    "        self.itos = {0: '[PAD]', 1: '[SOS]', 2: '[EOS]', 3: '[UNK]'}\n",
    "        self.stoi = {'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, '[UNK]': 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        \n",
    "        self.pad_id = self.stoi['[PAD]']\n",
    "        self.sos_id = self.stoi['[SOS]']\n",
    "        self.eos_id = self.stoi['[EOS]']\n",
    "        self.oov_id = self.stoi['[UNK]']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        return self.stoi\n",
    "\n",
    "    def token_to_ids(self, tokens):\n",
    "        if isinstance(tokens, str): # handle a single word or sentence here\n",
    "            token_list = self.tokenizer(tokens)\n",
    "            return [self.stoi[t] if t in self.stoi else self.stoi['[UNK]'] for t in token_list]\n",
    "\n",
    "        elif isinstance(tokens, list):\n",
    "            return [self.stoi[t] if t in self.stoi else self.stoi['[UNK]'] for t in tokens]\n",
    "        \n",
    "        else:\n",
    "            raise TypeError(\"Input must be either String or List of words.\")\n",
    "\n",
    "    def ids_to_token(self, ids):\n",
    "        return [self.itos[id] for id in ids]\n",
    "\n",
    "    # building vocab with the input sentence list\n",
    "    def adapt(self, sentences, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        idx = len(self.itos)\n",
    "        token_freqs = {}\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for token in self.tokenizer(sentence):\n",
    "                if token not in self.stoi:\n",
    "                    token_freqs[token] = 1\n",
    "                else:\n",
    "                    token_freqs[token] += 1\n",
    "                \n",
    "                if (token_freqs[token] == self.freq_threshold) and (idx < self.max_vocab_size):\n",
    "                    self.itos[idx] = token\n",
    "                    self.stoi[token] = idx\n",
    "                    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a7e967b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000, 13000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# english vocabulary\n",
    "en_vocab = Vocabulary(freq_threshold = 1, max_vocab_size = config['MAX_VOCAB_SIZE'])\n",
    "en_vocab.adapt(context_en, tokenizer)\n",
    "\n",
    "# portuguese vocabulary\n",
    "por_vocab = Vocabulary(freq_threshold = 1, max_vocab_size = config['MAX_VOCAB_SIZE'])\n",
    "por_vocab.adapt(target_por, tokenizer)\n",
    "\n",
    "en_vocab.vocab_size(), por_vocab.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a8c2901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here I am. ---------> Aqui estou.\n",
      "\n",
      "Encoder Input IDs: \n",
      "[1, 194, 20, 62, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Pre-Attention Decoder Input IDs (Shifted to the Right): \n",
      "[1, 402, 47, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Post-Attention Decoder Input IDs: \n",
      "[402, 47, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_idx = 789\n",
    "en_translation = context_en[test_idx]\n",
    "por_translation = target_por[test_idx]\n",
    "\n",
    "print(en_translation, '--------->', por_translation)\n",
    "\n",
    "max_seq_len = 16\n",
    "context_tokens = en_vocab.token_to_ids(en_translation)\n",
    "target_tokens = por_vocab.token_to_ids(por_translation)\n",
    "\n",
    "print(\"\\nEncoder Input IDs: \")\n",
    "print([en_vocab.sos_id] + context_tokens + [en_vocab.eos_id] + (max_seq_len - len(context_tokens) - 2) * [en_vocab.pad_id])\n",
    "print(\"\\nPre-Attention Decoder Input IDs (Shifted to the Right): \")\n",
    "print([en_vocab.sos_id] + target_tokens + [en_vocab.eos_id] + (max_seq_len - len(context_tokens) - 2) * [en_vocab.pad_id])\n",
    "print(\"\\nPost-Attention Decoder Input IDs: \")\n",
    "print(target_tokens + [en_vocab.eos_id] + (max_seq_len - len(context_tokens) - 1) * [en_vocab.pad_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaf6470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMT_dataset(Dataset):\n",
    "    def __init__(self, translation_pairs, tokenizers, vocabularies, max_seq_len):\n",
    "        self.translation_pairs = translation_pairs\n",
    "        self.en_tokenizer, self.por_tokenizer = tokenizers\n",
    "        self.en_vocab, self.por_vocab = vocabularies\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # for convenience \n",
    "        self.sos_id = self.en_vocab.sos_id\n",
    "        self.eos_id = self.en_vocab.eos_id\n",
    "        self.pad_id = self.en_vocab.pad_id\n",
    "        self.oov_id = self.en_vocab.oov_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.translation_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        req_pair = self.translation_pari[idx]\n",
    "        en_translation, por_translation = req_pair\n",
    "\n",
    "        context_tokens = self.en_vocab.token_to_ids(en_translation)\n",
    "        target_tokens = self.por_vocab.token_to_ids(por_translation)\n",
    "\n",
    "        # encoder input tokens\n",
    "        encoder_input = (\n",
    "            [self.sos_id] + \n",
    "            context_tokens + \n",
    "            [self.eos_id] + \n",
    "            (self.max_seq_len - len(context_tokens) - 2) * [self.pad_id]\n",
    "            )\n",
    "        \n",
    "        # pre-attention decoder input tokens\n",
    "        pre_decoder_input = (\n",
    "            [self.sos_id] + \n",
    "            target_tokens + \n",
    "            [self.eos_id] + \n",
    "            (max_seq_len - len(context_tokens) - 2) * [self.pad_id] \n",
    "        )\n",
    "\n",
    "        # post-attention decoder output tokens\n",
    "        post_decoder_output = (\n",
    "            target_tokens + \n",
    "            [self.eos_id] + \n",
    "            (max_seq_len - len(context_tokens) - 1) * [self.pad_id]\n",
    "        )\n",
    "\n",
    "        encoder_input_tensor = torch.tensor(encoder_input, dtype = torch.long)\n",
    "        pre_decoder_input_tensor = torch.tensor(pre_decoder_input, dtype = torch.long)\n",
    "        post_decoder_output_tensor = torch.tensor(post_decoder_output, dtype = torch.long)\n",
    "\n",
    "        return encoder_input_tensor, pre_decoder_input_tensor, post_decoder_output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66997a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad5ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
