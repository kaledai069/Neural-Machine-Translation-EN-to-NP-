{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aab66d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x2028e875b10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pathlib\n",
    "import re\n",
    "import unicodedata\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import sentencepiece as spm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7903db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'MAX_VOCAB_SIZE': 15000,\n",
    "    'BATCH_SIZE': 8,\n",
    "    'train_en_path': \"./dataset/consolidated/train.en\",\n",
    "    'train_ne_path': \"./dataset/consolidated/train.ne\",\n",
    "    'MAX_SEQ_LEN': 32,\n",
    "    'BUFFER_SIZE': 1000,\n",
    "    'UNITS': 256,\n",
    "    'EPOCHS': 10,\n",
    "    'DEVICE': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f39b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['train_en_path'], 'r', encoding='utf-8') as f:\n",
    "    en_lines = f.readlines()\n",
    "    \n",
    "with open(config['train_ne_path'], 'r', encoding='utf-8') as f:\n",
    "    ne_lines = f.readlines()\n",
    "    \n",
    "context_en = np.array(en_lines)\n",
    "target_ne = np.array(ne_lines)\n",
    "sentences = np.array((context_en, target_ne))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "908de7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained BPE tokenizers\n",
    "sp_en = spm.SentencePieceProcessor(model_file = 'en_bpe_model.model')\n",
    "sp_ne = spm.SentencePieceProcessor(model_file = 'ne_bpe_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f97b050e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_ne.piece_to_id('<unk>'), sp_ne.piece_to_id('<s>'), sp_ne.piece_to_id('</s>'), sp_ne.piece_to_id('<pad>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c2572",
   "metadata": {},
   "source": [
    "### Neural Machine Translation (EN - NE Translation Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eea78539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMT_dataset(Dataset):\n",
    "    def __init__(self, translation_pairs, src_tokenizer, tgt_tokenizer, max_seq_len, device = 'cpu'):\n",
    "    \n",
    "        self.translation_pairs = translation_pairs\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device\n",
    "\n",
    "        # for convenience \n",
    "        self.sos_id = self.src_tokenizer.bos_id()\n",
    "        self.eos_id = self.src_tokenizer.eos_id()\n",
    "        self.pad_id = self.src_tokenizer.piece_to_id('<pad>') # sentence_piece uses pad_id (-1)\n",
    "        self.oov_id = self.src_tokenizer.unk_id()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.translation_pairs.shape[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        req_pair = self.translation_pairs[:, idx]\n",
    "        \n",
    "        # src_translation -> English; tgt_translation -> Nepali (for this case)\n",
    "        src_translation, tgt_translation = req_pair\n",
    "\n",
    "        context_tokens = self.src_tokenizer.encode(src_translation, out_type = int)\n",
    "        target_tokens = self.tgt_tokenizer.encode(tgt_translation, out_type = int)\n",
    "\n",
    "        # encoder input tokens\n",
    "        encoder_input = (\n",
    "            [self.sos_id] + \n",
    "            context_tokens + \n",
    "            [self.eos_id] + \n",
    "            (self.max_seq_len - len(context_tokens) - 2) * [self.pad_id]\n",
    "            )\n",
    "        \n",
    "        # pre-attention decoder input tokens\n",
    "        pre_decoder_input = (\n",
    "            [self.sos_id] + \n",
    "            target_tokens + \n",
    "            [self.eos_id] + \n",
    "            (self.max_seq_len - len(target_tokens) - 2) * [self.pad_id] \n",
    "        )\n",
    "\n",
    "        # post-attention decoder output tokens\n",
    "        post_decoder_output = (\n",
    "            target_tokens + \n",
    "            [self.eos_id] + \n",
    "            (self.max_seq_len - len(target_tokens) - 1) * [self.pad_id]\n",
    "        )\n",
    "\n",
    "        encoder_input_tensor = torch.tensor(encoder_input[:self.max_seq_len], dtype = torch.long).to(self.device)\n",
    "        pre_decoder_input_tensor = torch.tensor(pre_decoder_input[:self.max_seq_len], dtype = torch.long).to(self.device)\n",
    "        post_decoder_output_tensor = torch.tensor(post_decoder_output[:self.max_seq_len], dtype = torch.long).to(self.device)\n",
    "\n",
    "        return encoder_input_tensor, pre_decoder_input_tensor, post_decoder_output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3f0a3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 128972) (2, 22965)\n"
     ]
    }
   ],
   "source": [
    "# train and val_dataset\n",
    "is_train = np.random.uniform(size = (sentences.shape[-1],)) < 0.85\n",
    "train_raw_set = sentences[:, is_train]\n",
    "val_raw_set = sentences[:, ~is_train]\n",
    "\n",
    "print(train_raw_set.shape, val_raw_set.shape)\n",
    "\n",
    "train_dataset = NMT_dataset(train_raw_set, sp_en, sp_ne, config['MAX_SEQ_LEN'])\n",
    "val_dataset = NMT_dataset(val_raw_set, sp_en, sp_ne, config['MAX_SEQ_LEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ebf6dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = config['BATCH_SIZE'], \n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size = config['BATCH_SIZE'], \n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10aa3296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # input embedding\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = vocab_size,\n",
    "            embedding_dim = units,\n",
    "            padding_idx = 3\n",
    "        )\n",
    "        \n",
    "        # bi-directional LSTM\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = units,\n",
    "            hidden_size = units,\n",
    "            batch_first = True,\n",
    "            bidirectional = True\n",
    "        )\n",
    "\n",
    "    def forward(self, context):\n",
    "        x = self.embedding(context)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = x[:, :, :self.rnn.hidden_size] + x[:, :, self.rnn.hidden_size:]\n",
    "\n",
    "        return x\n",
    "    \n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, units):\n",
    "\n",
    "        super(CrossAttention, self).__init__()\n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim = units,  # the size of Q, K, V dims is the embedding dimension\n",
    "            num_heads = 1,  \n",
    "            batch_first = True  # (batch_size, sequence_length, embedding_dim) either (seq_len, batch_size, embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(units)\n",
    "        \n",
    "        # to accumulate the inputs from encoder output (encoder_in) and the pre-attention decoder (target_in)\n",
    "        self.add = nn.ModuleList([nn.Linear(units, units) for _ in range(2)])\n",
    "\n",
    "    def forward(self, context, target):\n",
    "        # query is the target_in from the pre-attention decoder, while key/value are the context from the encoded context from the encoder\n",
    "        attn_output, _ = self.multihead_attn(query = target, key = context, value = context)\n",
    "        \n",
    "        x = self.add[0](target) + self.add[1](attn_output)\n",
    "        return self.layernorm(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, units, padding_idx = 0)\n",
    "        self.pre_attention_rnn = nn.LSTM(units, units, batch_first = True)\n",
    "        self.attention = CrossAttention(units)\n",
    "        self.post_attention_rnn = nn.LSTM(units, units, batch_first = True)\n",
    "        self.output_layer = nn.Linear(units, vocab_size)\n",
    "\n",
    "    def forward(self, context, target, state = None, return_state = False):\n",
    "        \n",
    "        x = self.embedding(target)\n",
    "        \n",
    "        # pre-attention-LSTM (decoder)\n",
    "        if state is None:\n",
    "            x, (hidden_state, cell_state) = self.pre_attention_rnn(x)\n",
    "        else:\n",
    "            x, (hidden_state, cell_state) = self.pre_attention_rnn(x, state)\n",
    "        \n",
    "        # cross-attention between pre-attention-LSTM output and the encoded context\n",
    "        x = self.attention(context, x)\n",
    "        \n",
    "        # post-attention-LSTM (decoder)\n",
    "        x, _ = self.post_attention_rnn(x)\n",
    "\n",
    "        # last linear (dense) layer\n",
    "        logits = self.output_layer(x)\n",
    "        logits = F.log_softmax(logits, dim = -1)\n",
    "\n",
    "        if return_state:\n",
    "            return logits, (hidden_state, cell_state)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class NMT_Translator(nn.Module):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        super(NMT_Translator, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size, units)\n",
    "        self.decoder = Decoder(vocab_size, units)\n",
    "        \n",
    "    def forward(self, context, target):\n",
    "        encoded_context = self.encoder(context)\n",
    "        logits = self.decoder(encoded_context, target) # this target here is the target_in (target_out is used for training)\n",
    "        \n",
    "        return logits # (batch_size, max_seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14305865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAcc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedAcc, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = y_pred.argmax(dim = -1)\n",
    "\n",
    "        mask = (y_true != 0).float()\n",
    "        correct = (y_true == y_pred).float() * mask\n",
    "        return correct.sum() / mask.sum()\n",
    "    \n",
    "class SparseCategoricalMaskedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SparseCategoricalMaskedLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        batch_size, seq_len, vocab_size = y_pred.shape\n",
    "    \n",
    "        # mask to avoid the padding sequence\n",
    "        # for loss mask\n",
    "        mask = (y_true != 0).float()\n",
    "\n",
    "        y_pred = y_pred.view(-1, y_pred.size(-1))\n",
    "        y_true = y_true.view(-1)\n",
    "        \n",
    "        loss = F.cross_entropy(y_pred, y_true.long(), reduction = 'none')\n",
    "\n",
    "        loss = loss.view(batch_size, seq_len)\n",
    "        loss *= mask\n",
    "\n",
    "        return loss.sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f2493d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = config['MAX_VOCAB_SIZE'] # for padding_idx\n",
    "units = config['UNITS']\n",
    "\n",
    "model = NMT_Translator(vocab_size, units)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "masked_acc_fn = MaskedAcc()\n",
    "\n",
    "sparse_masked_loss_fn = SparseCategoricalMaskedLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccc56492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m sparse_masked_loss_fn(output\u001b[38;5;241m.\u001b[39mfloat(), target_out\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     20\u001b[0m acc \u001b[38;5;241m=\u001b[39m masked_acc_fn(output\u001b[38;5;241m.\u001b[39mfloat(), target_out\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m---> 22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    523\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[0;32m    290\u001b[0m     tensors,\n\u001b[0;32m    291\u001b[0m     grad_tensors_,\n\u001b[0;32m    292\u001b[0m     retain_graph,\n\u001b[0;32m    293\u001b[0m     create_graph,\n\u001b[0;32m    294\u001b[0m     inputs,\n\u001b[0;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(config['EPOCHS']):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['EPOCHS']}\", leave = False)\n",
    "    for batch in train_loader_tqdm:\n",
    "        \n",
    "        # encoder input, pre-attention-decoder input and post-attention-decoder target output\n",
    "        context, target_in, target_out = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(context, target_in)\n",
    "\n",
    "        loss = sparse_masked_loss_fn(output.float(), target_out.float())\n",
    "        acc = masked_acc_fn(output.float(), target_out.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item()\n",
    "        train_loader_tqdm.set_postfix(loss = loss.item(), accuracy = acc.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_acc = total_acc / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{config['EPOCHS']}\")\n",
    "    print(f\"Training loss: {avg_loss:.4f}, accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            context, target_in, target_out = batch\n",
    "            output = model(context, target_in)\n",
    "            loss = sparse_masked_loss_fn(output.float(), target_out.float())\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= 3:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "# model.load_state_dict(torch.load('best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261403b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example embedding layer\n",
    "embedding = torch.nn.Embedding(num_embeddings = vocab_size, embedding_dim = units)\n",
    "\n",
    "# Example indices (replace with your actual input indices)\n",
    "input_indices = torch.tensor([    1,  4979,   803,  3175, 14939,    54,   382,     5,  2137,   114,\n",
    "          5162, 14940,   235,  7879,  8428,  1283, 10056,  6046,  2374, 14938,\n",
    "           358,   144,     5,   873,  7676,    33,  3878,    28,  1851,   261,\n",
    "          9110,  5308, -1])  # Notice 10000 is out of range\n",
    "\n",
    "# Forward pass (use only valid indices)\n",
    "output = embedding(input_indices)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
